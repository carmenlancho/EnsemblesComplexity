geom_vline(xintercept = 1.5, color = 'darkgray', linewidth = 0.5, linetype="dashed")+
scale_fill_manual(values=c('#006c4b', '#cde4b3','gray20', 'gray73')) +
theme(legend.position="none",axis.text.x = element_text(size = 11),
axis.title.x=element_blank(),axis.title.y=element_blank()) +
scale_x_discrete(labels=c("ay" = "y", "x1" = expression(x[1]),"x2"=expression(x[2]),
"x3" = expression(x[3]),"x4"=expression(x[4]),
"x5" = expression(x[5]),"x6"=expression(x[6]),
"x7" = expression(x[7]),"x8"=expression(x[8])))
print(ggplot(melted,aes(x=x,y=value,fill=interaction(variable,Color))) +
geom_bar(stat="identity",position = "identity", alpha=0.7))+ theme_light() +
geom_vline(xintercept = 1.5, color = 'darkgray', linewidth = 0.5, linetype="dashed")+
scale_fill_manual(values=c('#006c4b', '#cde4b3','gray20', 'gray73')) +
theme(legend.position="none",axis.text.x = element_text(size = 11),
axis.title.x=element_blank(),axis.title.y=element_blank()) +
scale_x_discrete(labels=c("ay" = "y", "x1" = expression(x[1]),"x2"=expression(x[2]),
"x3" = expression(x[3]),"x4"=expression(x[4]),
"x5" = expression(x[5]),"x6"=expression(x[6]),
"x7" = expression(x[7]),"x8"=expression(x[8]))) +
ylim(0,1)
err = 0.8
log(0.2/0.8)
log(0.8/0.2)
log(0.99/0.01)
exp(4.59)
exp(-4.59)
log((1-err)/err)
(1/100)*exp(-1.38)
(1/100)*exp(1.38)
(1/2)*log((1-err)/err)
(1/100)*exp(-1.38)
(1/100)*exp(1.38)
err=0.1
log((1-err)/err)
alpha = log((1-err)/err)
(1/500)*exp(alpha)
err=0.3
alpha = log((1-err)/err)
(1/500)*exp(alpha)
1/500
err=0.7
alpha = log((1-err)/err)
(1/500)*exp(alpha)
exp(-2)
exp(2)
28*2
200/3
200/5
200/9
200/13
200/17
200/21
200/25
200/29
200/33
200/37
200/41
200/45
200/49
200/53
200/57
67*3
5*40
23*9
16*13
12*17
21*10
21*9
8*25
7*33
6
7*29
6*33
7*33
6*37
5*41
5*45
5*49
4*49
4*45
6*33
5*37
4*53
53*3
4*57
3*57
35/2
17+17
200/35
6*35
5*35
s=c(1,2,4,6,8,10,12)
2*s+1
200/3
200/37
if (requireNamespace("thematic"))
thematic::thematic_rmd(font = "auto")
library(jsonlite)
library(ggplot2)
library(dplyr)
library(tidyr)
convert_time <- function(time_str) {
if (is.na(time_str)) {
return(NA)
}
time_parts <- unlist(strsplit(time_str, ":"))
if (length(time_parts) == 2) {
return(as.numeric(time_parts[1]) * 60 + as.numeric(time_parts[2]))
} else {
return(as.numeric(time_str))
}
}
file_path <- "/home/carmen/URJC/PROYECTOS/Sport_analytics/Atletismo_code/AT-1500-M-f--1--.RS5.json"
# Read the JSON file
data <- fromJSON(file_path)
str(data)
# Extract event details
event_details <- data.frame(
Event_Date = data$`Event Date`,
Temperature = data$Temperature,
Humidity = data$Humidity,
Start_Time = data$`Start Time`
)
event_details
# Extract the Athletes data frame
athletes <- data$Athletes
athletes
# Initialize an empty data frame to store all lap times
all_lap_times <- data.frame()
# Iterate over each athlete's data
for (athlete_name in unique(athletes$Name)) {
athlete_data <- athletes %>%
filter(Name == athlete_name)
# Extract lap times
lap_times <- athlete_data$`Lap Times`[[1]]
# Add Distance column
lap_times <- lap_times %>%
mutate(Distance = seq(from = 100, to = 1400, by = 100))
# Convert Time to numeric format for plotting
lap_times$Time <- sapply(lap_times$Time, convert_time)
# Add athlete_name as a column for grouping in ggplot
lap_times$athlete_name <- athlete_name
# Extract best time and final position from athlete_data
best_time <- athlete_data$`Best Time`
final_position <- athlete_data$`Final Position`
# Create a row with the best time and final position
best_time_row <- data.frame(Distance = 1500,
Time = convert_time(best_time),
athlete_name = athlete_name,
BestTime = best_time,
FinalPosition = final_position)
# Bind the best_time_row to lap_times
lap_times <- bind_rows(lap_times, best_time_row)
# Append to all_lap_times
all_lap_times <- bind_rows(all_lap_times, lap_times)
}
# Convert Time column to numeric format if it's not already numeric
all_lap_times$Time <- as.numeric(all_lap_times$Time)
p <- ggplot(all_lap_times, aes(x = Distance, y = Time, color = athlete_name)) +
geom_point() +
geom_line() +
labs(x = "Distance (meters)",
y = "Time (seconds)",
color = "Athlete") +
theme_minimal()
print(p)
# Imputamos los datos faltantes: tiempo/datos faltantes
all_lap_times[(all_lap_times$athlete_name=='Reynold Kipkorir  CHERUIYOT')&(all_lap_times$Distance %in% c(100,200,300)),2] = c(71.03/4,71.03*1/2,71.03*3/4)
p <- ggplot(all_lap_times, aes(x = Distance, y = Time, color = athlete_name)) +
geom_point() +
geom_line() +
labs(x = "Distance (meters)",
y = "Time (seconds)",
color = "Athlete") +
theme_minimal()
print(p)
library(dtw)
garcia = all_lap_times[(all_lap_times$athlete_name=='Mario  GARCÍA'),'Time']
kerr = all_lap_times[(all_lap_times$athlete_name=='Josh  KERR'),'Time']
plot(dtw(garcia,kerr,keep=TRUE), xlab="garcia - blue", ylab="kerr - magenta",type='threeway')
data_short = all_lap_times[,c(2,4,5)]
data_matrix = reshape(data_short, idvar = "athlete_name", timevar = "Distance", direction = "wide")
print(data_matrix)
data_matrix
rownames(data_matrix) <- data_matrix[,1]
rownames(data_matrix) = c("KERR","INGEBRIGTSEN","NORDAS","KIPSANG","NUGUSE","GARCIA","HOCKER","CHERUIYOT",
"GOURLEY","LAROS","HABZ","NADER")
data_matrix = data_matrix[-1]
data_matrix_T = t(data_matrix)
library(TSclust)
library(TSdist)
TSDatabaseDistances(data_matrix, distance="euclidean")
# position of every athlete
position = c(1:12)
# Perform MDS analysis
mds <- cmdscale(TSDatabaseDistances(data_matrix, distance="euclidean"))
# Plot the results
plot(mds[, 1], mds[, 2],
type = "n", xlab = "MDS Dimension 1",
ylab = "MDS Dimension 2")
# Plot the points and label them with
# the first two letters of the species name
points(mds[, 1], mds[, 2],
pch = 21, bg = "lightblue")
text(mds[, 1], mds[, 2],
#labels = substr(position, 1, 2),
labels = substr(rownames(mds), 1, 2),
pos = 1, cex = 0.8)
colnames(mds) <- c('x','y')
library(plotly)
mds_df = as.data.frame(mds)
# position of every athlete
mds_df$position = c(1:12)
fig <- plot_ly(data = mds_df, x = ~x, y = ~y,text = ~position)
fig <- fig %>% layout(title = 'MDS',
yaxis = list(zeroline = FALSE),
xaxis = list(zeroline = FALSE))
fig <- fig %>% add_markers()
fig <- fig %>% add_text( textposition = "top right")
fig
# Referencia de carrera rápida, lenta
ref <- seq(from = 14.3, by = 14.3,length.out = 15)
data_matrix_ref = t(t(data_matrix)<ref) # hay que transponer porque la comparación natural es por columnas
data_matrix_bin <- 1*data_matrix_ref # to binary
library(e1071)
dist_ham = hamming.distance(data_matrix_bin)
#perform hierachical clustering to the dist object
hc <- hclust(as.dist(dist_ham),method = 'complete')
#show the results
plot(hc)
data_matrix_bin
data_matrix_T
data_matrix
data_matrix
prcomp(data_matrix)
data_matrix
prcomp(data_matrix)
summary(prcomp(data_matrix))
diff(data_matrix)
data_matrix_dif %>%
mutate(data_matrix()[-1] - data_matrix()[-ncol(.)])
data_matrix %>%
mutate(cur_data()[-1] - cur_data()[-ncol(.)])
data_matrix_dif = data_matrix %>%
mutate(cur_data()[-1] - cur_data()[-ncol(.)])
prcomp(diff(data_matrix_dif))
prcomp(data_matrix_dif)
summary(prcomp(data_matrix_dif))
plot(prcomp(data_matrix_dif)$x[,1:2])
plot(prcomp(data_matrix_dif)$x[,1:2],type="n")
text(prcomp(data_matrix_dif,)$x[,1:2],rownames(data_matrix_dif))
data_matrix_dif[-c(2,4,8),]
plot(prcomp(data_matrix_dif[-c(2,4,8),])$x[,1:2],type="n")
text(prcomp(data_matrix_dif[-c(2,4,8),])$x[,1:2],rownames(data_matrix_dif[-c(2,4,8),]))
exp(0.5)
exp(-0.5)
exp(1)
exp(-1)
500*exp(-1)
1/500 * exp(1)
250/500 * exp(1)
400*10*10*3
400*10*3
47*2
10*16*47
10*16*47*2
a = 15012
15040-15012
28-16
10*16
160-146
4500*5
5*3*10*300
47*2
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm_cycles")
# datos <- read.csv('TotalAggregatedResults_ParameterConfiguration_CDB.csv')
# Cargamos los datos ya agregados por medida de complejidad porque si no peta
datos <- read.csv('df_summary_data.csv')
str(datos)
# Convert id and time into factor variables
datos <- datos %>%
convert_as_factor(Dataset, alpha,split, n_cycle,n_ensemble)
res.aov <- anova_test(
data = datos,formula = accuracy_mean ~ n_cycle*split*alpha,
dv = accuracy_mean, wid = Dataset,
within = c(n_cycle, split,alpha)
)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_mean ~ n_cycle*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(n_cycle, split,alpha)
)
datos <- read.csv('df_summary_data.csv')
str(datos)
datos <- datos %>%
convert_as_factor(Dataset, combo_alpha_split, n_cycle,n_ensemble)
friedman.test(datos, groups = n_cycle, blocks = combo_alpha_split)
datos
str(datos)
friedman.test(datos, groups = n_cycle, blocks = combo_alpha_split)
friedman.test(datos, groups = combo_alpha_split, blocks = n_cycle)
friedman.test(accuracy_mean_mean ~ combo_alpha_split | n_cycle, data= datos)
pairwise.wilcox.test(datos$accuracy_mean_mean, g = datos$n_cycle)
View(datos)
datos_a = datos[datos$combo_alpha_split=='alpha2-split1',]
datos_a
# no funciona porque no es un diseño completo
friedman.test(accuracy_mean_mean ~ n_cycle, data= datos_a)
res.aov <- anova_test(
data = datos_a,formula = accuracy_mean_mean ~ n_cycle,
dv = accuracy_mean_mean, wid = Dataset,
within = c(n_cycle)
)
get_anova_table(res.aov, correction = 'GG')
pairwise.wilcox.test(datos_a$accuracy_mean_mean, g = datos_a$n_cycle)
pwc_split <- datos_a %>%
pairwise_t_test(
accuracy_mean_mean ~ n_cycle, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc_split
View(pwc_split)
aa = pairwise.wilcox.test(datos_a$accuracy_mean_mean, g = datos_a$n_cycle)
aa
aa[100,]
aa[1,]
aa$data.name
aa$method
aa$p.value
aa$p.value[1,]
aa$p.value[100,]
aa$p.value[10,]
aa$p.value[9910,]
aa$p.value[99,]
datos <- read.csv('TotalAggregatedResults_ParameterConfiguration_CDB.csv')
datos <- datos %>%
convert_as_factor(Dataset, combo_alpha_split, n_cycle,n_ensemble,weights)
datos_a = datos[datos$combo_alpha_split=='alpha2-split1',]
res.aov <- anova_test(
data = datos_a,formula = accuracy_mean_mean ~ n_cycle*weights,
dv = accuracy_mean_mean, wid = Dataset,
within = c(n_cycle,weights)
)
res.aov <- anova_test(
data = datos_a,formula = accuracy_mean ~ n_cycle*weights,
dv = accuracy_mean, wid = Dataset,
within = c(n_cycle,weights)
)
# no funciona porque no es un diseño completo
friedman.test(accuracy_mean_mean ~ n_cycle, data= datos_a)
res.aov
### Residual Checks
res.aov$model
### Residual Checks
res.aov$ges$model
### Residual Checks
res.aov$ges
### Residual Checks
res.aov$ges$model$residuals
### Residual Checks
res.aov$ges[model]
### Residual Checks
res.aov$ges[0]
### Residual Checks
res.aov$ges[1]
### Residual Checks
res.aov$ges[2]
### Residual Checks
res.aov$ges[[1]]
### Residual Checks
res.aov$ges[[2]]
### Residual Checks
res.aov$ges[[1]][, "Residuals"]
args(res.aov$ges)
resid(res.aov, type = "pearson")
# Normality check
plot(res.aov)
summary(res.aov)
res = attributes(res.aov)$args$model$residuals
res
length(res)
index_sample <- sample(1:length(res),4000)
res_sample <- res[index_sample]
shapiro.test(as.numeric(res_sample))
hist(res)
pwc2 <- datos_a %>%
wilcox_test(accuracy_mean_mean ~ n_cycle, p.adjust.method = "bonferroni")
datos <- read.csv('df_summary_data.csv')
str(datos)
# Convert id and time into factor variables
datos <- datos %>%
convert_as_factor(Dataset, combo_alpha_split, n_cycle,n_ensemble)
datos_a = datos[datos$combo_alpha_split=='alpha2-split1',]
# no paramétrico
pwc2 <- datos_a %>%
wilcox_test(accuracy_mean_mean ~ n_cycle, p.adjust.method = "bonferroni")
pwc2 <- datos_a %>%
wilcox_test(accuracy_mean_mean ~ n_cycle, paired = TRUE, p.adjust.method = "bonferroni")
pwc2
View(pwc2)
pwc2
View(pwc2)
levels(combo_alpha_split)
levels(datos$combo_alpha_split)
for (i in levels(datos$combo_alpha_split)){
print(i)
}
for (i in levels(datos$combo_alpha_split)){
print(i)
datos_i = datos[datos$combo_alpha_split==i,]
print(datos_i)
}
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
res.fried
res.fried$p
# Tenemos que hacer el análisis para cada combo_alpha_split
valores_combo = levels(datos$combo_alpha_split)
valores_combo
n_combo = length(valores_combo)
n_combo
combo_friedman = data.frame(valores_combo)
combo_friedman
combo_friedman$p_value = rep(NA,n_combo)
combo_friedman
print(i)
combo_friedman[i,]
i
combo_friedman[combo_friedman$valores_combo==i,]
combo_friedman[combo_friedman$valores_combo==i,2]
valores_combo = levels(datos$combo_alpha_split)
n_combo = length(valores_combo)
combo_friedman = data.frame(valores_combo)
combo_friedman$p_value = rep(NA,n_combo)
for (i in valores_combo){
print(i)
datos_i = datos[datos$combo_alpha_split==i,]
print(datos_i)
# Friedman, no paramétrico
res.fried <- datos_i %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
combo_friedman[combo_friedman$valores_combo==i,2] = res.fried$p
}
combo_friedman
print(i)
datos_i = datos[datos$combo_alpha_split==i,]
# Friedman, no paramétrico
res.fried <- datos_i %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_a = datos_i
pwc2 <- datos_a %>%
wilcox_test(accuracy_mean_mean ~ n_cycle, paired = TRUE, p.adjust.method = "bonferroni")
View(datos_i)
datos_a = datos[datos$combo_alpha_split=='alpha10-split10',]
res.aov <- anova_test(
data = datos_a,formula = accuracy_mean_mean ~ n_cycle,
dv = accuracy_mean_mean, wid = Dataset,
within = c(n_cycle)
)
get_anova_table(res.aov, correction = 'GG')
# Friedman, no paramétrico
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_i <- datos_i %>%
convert_as_factor(Dataset, combo_alpha_split, n_cycle,n_ensemble)
# Friedman, no paramétrico
res.fried <- datos_i %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_i = datos[datos$combo_alpha_split==i,]
print(datos_i)
D_fri <- datos_i %>% dplyr::select(Dataset, accuracy_mean_mean, n_cycle)
D_fri <- as.data.frame(D_fri)
# Friedman, no paramétrico
res.fried <- D_fri %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
View(D_fri)
friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, D_fri)
datos_a = datos[datos$combo_alpha_split=='alpha8-split10',]
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_a = datos[datos$combo_alpha_split=='alpha2-split1',]
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
res.fried$p
datos_a = datos[datos$combo_alpha_split=='alpha2-split10',]
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_a = datos[datos$combo_alpha_split=='alpha2-split2',]
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
datos_a = datos[datos$combo_alpha_split=='alpha4-split1',]
res.fried <- datos_a %>% friedman_test(accuracy_mean_mean ~ n_cycle |Dataset)
res.fried$p
View(datos_a)
datos_i = datos_a
print(datos_i)
D_fri <- datos_i %>% dplyr::select(Dataset, accuracy_mean_mean, n_cycle)
D_fri <- as.data.frame(D_fri)
View(D_fri)
datos_a = datos[datos$combo_alpha_split=='alpha4-split2',]
View(datos_a)
friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, data=datos_a)
friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, data=as.matrix(datos_a))
aa = friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, data=as.matrix(datos_a))
aa$p.value
# Tenemos que hacer el análisis para cada combo_alpha_split
valores_combo = levels(datos$combo_alpha_split)
n_combo = length(valores_combo)
combo_friedman = data.frame(valores_combo)
combo_friedman$p_value = rep(NA,n_combo)
for (i in valores_combo){
print(i)
datos_i = datos[datos$combo_alpha_split==i,]
print(datos_i)
fri = friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, data=as.matrix(datos_i))
combo_friedman[combo_friedman$valores_combo==i,2] = fri$p.value
}
combo_friedman
