---
title: "CDB_cycles_AnalysisOfParameters"
editor: visual
format:
  html: 
    page-layout: full
    embed-resources: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
# Cargamos las librerias que vamos necesitando a lo largo del codigo
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
library(DT)
library(ggplot2)
library(patchwork)
library(plotly)
library(dplyr)
```

In this notebook, we are studying the different parameters of our method Complexity Driven Bagging so as to offer a range of selection to the final user. In particular, we have three parameters:

-   Split: the number of splits in which we cut the complexity spectrum. s=1 implies we are training with one easy sample, one uniform sample and one hard sample. That is, the cycle length is 3. s=2 implies 6 samples of different complexity (cycle length is 6).

-   Alpha: to give more weight to the easiest and the hardest instances in the bootstrap sampling procedure with the aim of training the classifier with samples of higher or lower complexity (thus, enlarging the original range of complexity).

-   Number of cycles. How many times the procedure is repeated. This is totally related with the final number of ensembles.

Besides these 3 parameters, we have obtained results for different complexity measures. For the analysis of the parameters, we have aggregated results over the different complexity measures.

First, we are studying, for each one of the parameters (alpha, split, number of cycles) independently, for which values there are no significant differences and, thus, can be eliminated from the range of recommended values.

In particular:

-   For alpha and split, we aggregate over the rest of parameters since the number of tested models depends on the value of split. Therefore, for low split values, there are a lot of results but for high split values there are just a few (1000 vs 50). This requires special statistical test because the design is imbalanced. Furthermore, the comparison will not be fair since the number of models is indeed the same (we have used around 300 models in all cases) but when split is low, the number of cycles is higher and more results are saved (for example 1000) and when split is high there are only a few cycles (for example 50) but representing the same amount of models. Thus, to make the comparison fair (and not play with different samples sizes and variances), we aggregate over the rest of the results.

-   For the number of cycles, we compared the number of cycles for every combination of split and alpha. The idea is that, given a value of split and a value of alpha, we want to know when the best accuracy is obtained, when there are significant differences, etc. so as to recommend the lower number of cycles (lower number of ensembles) with the best performance. We start by identifying, for each combination of split and alpha, from what number of cycles onwards there is no significant difference in the accuracy obtained.

After this analysis, we will have a first range recommendation for every parameter. Notice that, in all cases, we take into account the mean, median and standard deviation of the accuracy.

# Parameter analysis

```{r warning=FALSE, include=FALSE}

getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm_cycles")
#datos <- read.csv('TotalAggregatedResults_ParameterConfiguration_CDB.csv') 
# Data aggregated over complexity measures
datos <- read.csv('df_summary_data.csv') 
str(datos)
# Como en python empezamos en 0, tenemos que sumar 1 a n_ensemble
datos$n_ensemble <- datos$n_ensemble + 1
# Convert id and time into factor variables
datos <- datos %>%
  convert_as_factor(Dataset, combo_alpha_split, n_cycle,n_ensemble)
```

### Mean, median and standard deviation of accuracy for all levels of split

```{r warning=FALSE}
table_split <- datos %>%
  group_by(split) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
knitr::kable(table_split)
```

```{r echo=FALSE, warning=FALSE}

# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_split, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$split <- as.numeric(df_long$split)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = split, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "split", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = split, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "split", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)

```

The higher the value of split, the higher the mean (with some exceptions) of accuracy, the lower the median and the lower the standard deviation. ¿Medium-low split values?

If we compare if there are significant differences among the different split values (once aggregated per n_cycle). We obtain that:

For the **mean of the accuracy**, there are no significant differences among:

-   4 with 6, 6 with 8 and 12

-   8 with 12,14,16

-   10 with 12, 14, 18, 22

-   12 with 14, 16, 18, 20, 22

-   14 with 16, 18, 20, 22, 26

-   From 16 to 30, almost all comparisons are not significantly different --\> maximum value of split should be 16

For the **median of the accuracy**, there are no significant differences among:

-   4 with 6 and 10

-   6 with 8, 10, 12

-   8 with 10, 12, 14, 16, 18, 20, 22

-   10 with 12, 14, 16, 18, 20, 22

-   12 with 14, 16, 18, 20, 22

-   14 with 16, 18, 20, 22, 26

-   From 16 to 30, almost all comparisons are not significantly different --\> maximum value of split should be 16

For the **std of the accuracy**, there are no significant differences among:

-   4 with 6 and 8

-   6 with 8, 10, 12

-   8 with 10, 12, 14, 16, 20

-   10 with 12, 14, 16, 20, 22, 30

-   12 with 14, 16, 20, 22, 26, 30

-   From 14 to 30, almost all comparisons are not significantly different --\> maximum value of split should be 14

### Mean, median and standard deviation of accuracy for all levels of alpha

```{r warning=FALSE}
table_alpha <- datos %>%
  group_by(alpha) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
knitr::kable(table_alpha)
```

```{r echo=FALSE, warning=FALSE}

# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_alpha, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$alpha <- as.numeric(df_long$alpha)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = alpha, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "alpha", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = alpha, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "alpha", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)
```

The higher the value of alpha, the lower the mean and the median of accuracy. The standard deviation keeps lower for low-medium values. --\> Low-medium values of alpha. Lower than 12.

If we compare if there are significant differences among the different alpha values (once aggregated per n_cycle). We obtain that:

For the **mean of the accuracy**, there are ONLY significant differences among:

-   2 with 10

-   10 with 12, 14, 16, 18, 20

For the **median of the accuracy**, there are ONLY significant differences among:

-   2 with 4, 6, 8, 10, 14

-   10 with 12, 16, 20

For the **std of the accuracy**, there are NO significant differences among:

-   4 with 6 and 8

-   6 with 8, 10, 12

-   8 with 10, 12

-   From 10 to 20, almost all comparisons are not significantly different --\> maximum value of alpha should be 10

### Mean, median and standard deviation of accuracy for all levels of n_cycles (for some split values)

We cannot perform a summary of 'n_cycle' in general because the number of cycles depends on the value of split. Thus, we show some cases.

**split = 1**

```{r warning=FALSE}
table_split1 <- datos %>% filter(split == 1) %>%
  group_by(n_cycle) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
#knitr::kable(table_split1)

#datatable(table_split1)
## solo para html
#library(DT)

## Crear una tabla interactiva con paginación
#datatable(table_split1, 
#          options = list(pageLength = 15, # Muestra 15 filas por página
#                         lengthMenu = c(15, 30, 50, 100), # Opciones de filas por página
#                         autoWidth = TRUE))


```

```{r echo=FALSE, warning=FALSE}

# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_split1, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$n_cycle <- as.numeric(df_long$n_cycle)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)

```

The higher the number of cycles, the higher the mean, median of accuracy and the lower the standard deviation. For high values of cycles, the accuracy clearly stabilizes and there is no always a clear increase over time. For example, results with 89 cycles are better than with 100.

**split = 2**

```{r warning=FALSE}
table_split2 <- datos %>% filter(split == 2) %>%
  group_by(n_cycle) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
knitr::kable(table_split2)
```

```{r echo=FALSE, warning=FALSE}
# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_split2, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$n_cycle <- as.numeric(df_long$n_cycle)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)

```

The higher the number of cycles, the higher the mean, median of accuracy and the lower the standard deviation. For high values of cycles, the accuracy clearly stabilizes and there is no always a clear increase over time.

**split = 4**

```{r warning=FALSE}
table_split4 <- datos %>% filter(split == 4) %>%
  group_by(n_cycle) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
knitr::kable(table_split4)
```

```{r echo=FALSE, warning=FALSE}
# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_split4, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$n_cycle <- as.numeric(df_long$n_cycle)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)

```

The higher the number of cycles, the higher the mean, median of accuracy and the lower the standard deviation. For high values of cycles, the accuracy stabilizes but keeps showing an increasing trend. The longest the cycle, the less stable the trend (still increasing).

**split = 10**

```{r warning=FALSE}
table_split10 <- datos %>% filter(split == 10) %>%
  group_by(n_cycle) %>%
  summarise_at(vars(accuracy_mean_mean),  list(mean = mean, median = median, std = sd))
knitr::kable(table_split10)
```

```{r echo=FALSE, warning=FALSE}
# Convertir el dataframe al formato largo para ggplot
df_long <- tidyr::pivot_longer(table_split10, cols = c("mean", "median", "std"), 
                               names_to = "variable", values_to = "value")

df_long$n_cycle <- as.numeric(df_long$n_cycle)

# Crear el primer gráfico (mean y median) con ajuste en los breaks del eje x
g1 <- ggplot(df_long[df_long$variable %in% c("mean", "median"),], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution mean median",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("mean" = "purple", "median" = "orange"))

# Crear el segundo gráfico (std) con ajuste en los breaks del eje x
g2 <- ggplot(df_long[df_long$variable == "std",], 
             aes(x = n_cycle, y = value, color = variable)) +
  geom_line(size = 1) +
  geom_point(size = 1) +
  labs(title = "Evolution std",
       x = "n_cycle", y = "Value", color = " ") +
  theme_minimal() +
  scale_color_manual(values = c("std" = "blue"))

# Colocar los dos gráficos juntos con patchwork
g1 + g2 + plot_layout(nrow = 2)

```

The higher the number of cycles, the higher the mean, median of accuracy and the lower the standard deviation. For high values of cycles, the accuracy stabilizes but keeps showing an increasing trend. The longest the cycle, the less stable the trend (still increasing).

# Number of cycles

```{r warning=FALSE}
# Tenemos que hacer el análisis para cada combo_alpha_split
valores_combo = levels(datos$combo_alpha_split)
n_combo = length(valores_combo)
combo_friedman = data.frame(valores_combo)
combo_friedman$p_value = rep(NA,n_combo)

for (i in valores_combo){
  #print(i)
  datos_i = datos[datos$combo_alpha_split==i,]
  fri = friedman.test(accuracy_mean_mean ~ n_cycle |Dataset, data=as.matrix(datos_i))
  combo_friedman[combo_friedman$valores_combo==i,2] = fri$p.value
}
combo_friedman[combo_friedman$p_value> 0.05]
# es decir, en todos los casos hay diferencias significativas
```

Once we have checked that there are significant differences between at least one value in the combo, we make multiple comparisons to analyze when adding another cycle is not worthy since the increase is not significant.

```{r warning=FALSE}

dif_no_sig <- data.frame(valores_combo)
dif_no_sig$niveles = rep(NA,n_combo)

# Lo dejamos en comentarios porque tarda mucho

# for (i in valores_combo){
#   print(i)
#   datos_i = datos[datos$combo_alpha_split==i,]
#   datos_i$n_cycle <- factor(datos_i$n_cycle) # los niveles del factor cambian en cada subset
#   pwc2 <- datos_i %>% 
#     wilcox_test(accuracy_mean_mean ~ n_cycle, paired = TRUE, p.adjust.method = "bonferroni")
#   # Filtrar comparaciones con diferencias no significativas (suponiendo un umbral de p > 0.05)
#   no_significativas <- pwc2[pwc2$p.adj>0.1,]
# 
#   
#   # si no todas las comparaciones con ese nivel son no significativas, lo quitamos 
#   # es decir, no nos vale que solo no haya diferencia entre 3 y 5 y con el resto (3-6,3-7,etc) sí
#   max_cycles = max(as.numeric(pwc2$group2))
#   valores_check <- unique(as.numeric(no_significativas$group1))
#   for (v in valores_check){
#     if (sum(no_significativas$group1 == v) <(max_cycles - v) ){
#       no_significativas = no_significativas[no_significativas$group1!=v,]
#     }
#   }
#   
#   # Extraer los niveles de los pares con diferencias no significativas
#   niveles_no_significativos <- unique(c(no_significativas$group1, no_significativas$group2))
# 
#   dif_no_sig[dif_no_sig$valores_combo==i,2] = paste(niveles_no_significativos, collapse = ", ")
# }

#write.csv(dif_no_sig, "CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean.csv")
```

In this dataframe we have, for every combination of alpha and split, the number of cycles with no significant difference between all of them.

```{r warning=FALSE}
dif_no_sig_mean <- read.csv('CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean.csv') 
head(dif_no_sig_mean)
```

Let's relate that with the number of models to have a first view of where to stop adding models. We create two different columns:

-   num_models: it relates the first (minimum) number of cycles that presents no significant differences with all its consecutive number of cycles with the number of models it implies.

-   num_models2: it is exactly the same concept as num_models but with the second value of number of cycles in case we want to be more conservative

```{r}
# Variables to character
dif_no_sig_mean$niveles <- as.character(dif_no_sig_mean$niveles)
dif_no_sig_mean$valores_combo <- as.character(dif_no_sig_mean$valores_combo)

# Order the values 
dif_no_sig_mean$niveles <- sapply(strsplit(dif_no_sig_mean$niveles, ", "), function(x) {
  paste(sort(as.numeric(x)), collapse = ", ")
})

# Extraer el valor numérico después de "split" en la columna B
dif_no_sig_mean$valor_split <- as.numeric(gsub(".*split", "", dif_no_sig_mean$valores_combo))

# New columns with number of models
dif_no_sig_mean$num_models <- mapply(function(a, b) {
  min(as.numeric(strsplit(a, ", ")[[1]])) * (2*b +1)
}, dif_no_sig_mean$niveles, dif_no_sig_mean$valor_split)

# New columns with number of models (for the second value)
dif_no_sig_mean$num_models2 <- mapply(function(a, b) {
  valores <- sort(as.numeric(strsplit(a, ", ")[[1]])) 
  segundo_min <- ifelse(length(valores) > 1, valores[2], valores[1])  # Obtener el segundo mínimo o el primero si hay solo uno
  segundo_min * (2*b +1)
}, dif_no_sig_mean$niveles, dif_no_sig_mean$valor_split)

head(dif_no_sig_mean)
```

We perform the same analysis for the median and the standard deviation.

For the **median**:

```{r}
# dif_no_sig_mean$niveles_mediana = rep(NA,n_combo)
# 
# # Lo dejamos en comentarios porque tarda mucho
# 
# for (i in valores_combo){
#   print(i)
#   datos_i = datos[datos$combo_alpha_split==i,]
#   datos_i$n_cycle <- factor(datos_i$n_cycle) # los niveles del factor cambian en cada subset
#   pwc2 <- datos_i %>%
#     wilcox_test(accuracy_mean_median ~ n_cycle, paired = TRUE, p.adjust.method = "bonferroni")
#   # Filtrar comparaciones con diferencias no significativas (suponiendo un umbral de p > 0.05)
#   no_significativas <- pwc2[pwc2$p.adj>0.1,]
# 
# 
#   # si no todas las comparaciones con ese nivel son no significativas, lo quitamos
#   # es decir, no nos vale que solo no haya diferencia entre 3 y 5 y con el resto (3-6,3-7,etc) sí
#   max_cycles = max(as.numeric(pwc2$group2))
#   valores_check <- unique(as.numeric(no_significativas$group1))
#   for (v in valores_check){
#     if (sum(no_significativas$group1 == v) <(max_cycles - v) ){
#       no_significativas = no_significativas[no_significativas$group1!=v,]
#     }
#   }
# 
#   # Extraer los niveles de los pares con diferencias no significativas
#   niveles_no_significativos <- unique(c(no_significativas$group1, no_significativas$group2))
# 
#   dif_no_sig_mean[dif_no_sig_mean$valores_combo==i,'niveles_mediana'] = paste(niveles_no_significativos, collapse = ", ")
# }

#write.csv(dif_no_sig_mean, "CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean_median.csv")
```

For the **standard deviation**:

```{r}
# dif_no_sig_mean$niveles_std = rep(NA,n_combo)
# 
# # Lo dejamos en comentarios porque tarda mucho
# 
# for (i in valores_combo){
#   print(i)
#   datos_i = datos[datos$combo_alpha_split==i,]
#   datos_i$n_cycle <- factor(datos_i$n_cycle) # los niveles del factor cambian en cada subset
#   pwc2 <- datos_i %>%
#     wilcox_test(accuracy_mean_std ~ n_cycle, paired = TRUE, p.adjust.method = "bonferroni")
#   # Filtrar comparaciones con diferencias no significativas (suponiendo un umbral de p > 0.05)
#   no_significativas <- pwc2[pwc2$p.adj>0.1,]
# 
# 
#   # si no todas las comparaciones con ese nivel son no significativas, lo quitamos
#   # es decir, no nos vale que solo no haya diferencia entre 3 y 5 y con el resto (3-6,3-7,etc) sí
#   max_cycles = max(as.numeric(pwc2$group2))
#   valores_check <- unique(as.numeric(no_significativas$group1))
#   for (v in valores_check){
#     if (sum(no_significativas$group1 == v) <(max_cycles - v) ){
#       no_significativas = no_significativas[no_significativas$group1!=v,]
#     }
#   }
# 
#   # Extraer los niveles de los pares con diferencias no significativas
#   niveles_no_significativos <- unique(c(no_significativas$group1, no_significativas$group2))
# 
#   dif_no_sig_mean[dif_no_sig_mean$valores_combo==i,'niveles_std'] = paste(niveles_no_significativos, collapse = ", ")
# }

#write.csv(dif_no_sig_mean, "CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean_median_std.csv")
```

Now we relate the number of cycles with the number of ensembles for all statistical measures (mean, median, std):

```{r warning=FALSE}
dif_no_sig_all <- read.csv('CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean_median_std.csv') 
head(dif_no_sig_all)
```

```{r warning=FALSE}
# Variables to character
dif_no_sig_all$niveles_mediana <- as.character(dif_no_sig_all$niveles_mediana)
dif_no_sig_all$niveles_std <- as.character(dif_no_sig_all$niveles_std)
dif_no_sig_all$valores_combo <- as.character(dif_no_sig_all$valores_combo)

# Order the values 
dif_no_sig_all$niveles_mediana <- sapply(strsplit(dif_no_sig_all$niveles_mediana, ", "), function(x) {
  paste(sort(as.numeric(x)), collapse = ", ")
})

dif_no_sig_all$niveles_std <- sapply(strsplit(dif_no_sig_all$niveles_std, ", "), function(x) {
  paste(sort(as.numeric(x)), collapse = ", ")
})


# New columns with number of models
dif_no_sig_all$num_models_mediana <- mapply(function(a, b) {
  min(as.numeric(strsplit(a, ", ")[[1]])) * (2*b +1)
}, dif_no_sig_all$niveles_mediana, dif_no_sig_all$valor_split)

dif_no_sig_all$num_models_std <- mapply(function(a, b) {
  min(as.numeric(strsplit(a, ", ")[[1]])) * (2*b +1)
}, dif_no_sig_all$niveles_std, dif_no_sig_all$valor_split)

# New columns with number of models (for the second value)
dif_no_sig_all$num_models2_mediana <- mapply(function(a, b) {
  valores <- sort(as.numeric(strsplit(a, ", ")[[1]])) 
  segundo_min <- ifelse(length(valores) > 1, valores[2], valores[1])  # Obtener el segundo mínimo o el primero si hay solo uno
  segundo_min * (2*b +1)
}, dif_no_sig_all$niveles_mediana, dif_no_sig_all$valor_split)

dif_no_sig_all$num_models2_std <- mapply(function(a, b) {
  valores <- sort(as.numeric(strsplit(a, ", ")[[1]])) 
  segundo_min <- ifelse(length(valores) > 1, valores[2], valores[1])  # Obtener el segundo mínimo o el primero si hay solo uno
  segundo_min * (2*b +1)
}, dif_no_sig_all$niveles_std, dif_no_sig_all$valor_split)

# Sacamos tb el valor en ciclos
dif_no_sig_all$cycles_mean <- sapply(strsplit(dif_no_sig_all$niveles, ", "), function(x) {
  min(as.numeric(x))})

dif_no_sig_all$cycles_median <- sapply(strsplit(dif_no_sig_all$niveles_mediana, ", "), function(x) {
  min(as.numeric(x))})

dif_no_sig_all$cycles_std <- sapply(strsplit(dif_no_sig_all$niveles_std, ", "), function(x) {
  min(as.numeric(x))})


head(dif_no_sig_all)

#write.csv(dif_no_sig_all, "CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean_median_std_num_models.csv")
```

We have performed multiple comparisons analysis for the number of cycles with respect to the mean, median and std of the accuracy and found the number of cycles above which there is no significant difference for each statistical measure. Now we have to take the maximum number of cycles over the three statistical measure to select a range of cycles. After that we can analyze which is the best cycle structure (for example, a high number of short cycles or a short number of long cycles).

```{r}
dif_no_sig_all$max_num_cycles <- apply(X=dif_no_sig_all[,c('cycles_mean','cycles_median','cycles_std')], MARGIN=1, FUN=max)
dif_no_sig_all$max_num_models <- apply(X=dif_no_sig_all[,c('num_models','num_models_mediana','num_models_std')], MARGIN=1, FUN=max)

#write.csv(dif_no_sig_all, "CDB_cycles_ParametersComboAlphaSplit_dif_no_signif_cycles_mean_median_std_num_models.csv")
```

If we analyze the number of models above which there are no significant different, we can see that the maximum value is 294 and quartile 75% is 175, implying that our maximum tested number of models (300) is ok and that lower number of models in an ensemble can obtain competitive accuracy results.

```{r warning=FALSE}
p<-ggplot(dif_no_sig_all, aes(x=max_num_models)) + 
  geom_histogram(color="black", fill="white")
p
```

```{r}
summary(dif_no_sig_all$max_num_models)
```

From the study of alpha and split we know that:

-   From 16 to 30, almost all comparisons are not significantly different --\> **maximum value of split should be 16. Domain: \[1, 2, 4, 6, 8, 10, 12, 14\]**

-   Maximum alpha value should be 10-12: **Domain: \[2, 4, 6, 8, 10\]**

Note that these ranges go inline with the previous study where we make no distinction about cycles.

Let's filter now the previous information according to these ranges:

```{r}
dif_no_sig_all$valor_alpha <- as.numeric(gsub("alpha([0-9]+)-split[0-9]+", "\\1", dif_no_sig_all$valores_combo))
# Filtrar el dataset para eliminar las filas donde alpha > 12 y split > 16
df_filtered <- dif_no_sig_all[(dif_no_sig_all$valor_alpha < 12 & dif_no_sig_all$valor_split < 16), ]
```

```{r}
summary(df_filtered$max_num_models)
```

After filtering the not desired values for alpha and split, the maximum number of models where the significant differences stop is 200.

Este df_filtered son 40 filas, es decir, 40 combinaciones de alpha y split. Entonces vamos a estudiarlos 1 a 1. Hago un gráfico de la evolución del accuracy en cada caso y señalo cuando se supone que no hay diferencias significativas. Esto lo hacemos de nuevo agregado por medidas de complejidad y luego elijo un par y lo visualizo para ellas. Aquí estoy un poco perdida porque en general se ve que según aumenta el número de modelos, aumenta el accuracy pero supuestamente ya no de forma significativa y deberíamos guiarnos por eso. Yo creo que el orden es:

1.  Hacer estos gráficos de evolución del accuracy para los 40 casos y mirarlo para alguna medida de complejidad

2.  Seleccionar el mejor valor de accuracy para estos 40 casos

3.  Sacar conclusiones de la comparación de 1 y 2

4.  Para nuestro método, escoger el mejor valor de parámetros en 2 casos:

    1.  alpha, split y n_cycles reducido en función de las comparaciones múltiples

    2.  alpha y split reducido en función de las comparaciones múltiples y n_cycles sin reducir

    Comparar estas 2 versiones nuestras standard bagging y mixed bagging con el mismo número de parámetros

```{r}
datos_alpha_s <- datos %>% filter(alpha<12, split <16) 
datos_alpha_s <- datos_alpha_s %>% group_by(alpha, split, n_cycle, n_ensemble) %>%
  summarise_at(vars(accuracy_mean_mean),  list(accuracy_mean_dataset_mean = mean, accuracy_mean_dataset_median = median, accuracy_mean_dataset_std = sd))

```

For each combination of alpha and split we are plotting, with an orange dot, the maximum accuracy achieved and, with a blue dot, the number of ensembles from which there are no significant differences. In this case, the first thing to outline is that the orange point is not achieved at the maximum number of models tried (aroung 300), meaning that not always more models imply better performance. The blue dot appears quite before.

```{r}
datos_alpha_s_1 <- datos_alpha_s %>% filter(alpha==2, split==1)
datos_alpha_s_1 <- as.data.frame(datos_alpha_s_1)
datos_alpha_s_1$n_cycle <- as.numeric(as.character(datos_alpha_s_1$n_cycle))
datos_alpha_s_1$n_ensemble <- as.numeric(as.character(datos_alpha_s_1$n_ensemble))

idmax = which.max(datos_alpha_s_1$accuracy_mean_dataset_mean)
# max(datos_alpha_s_1$accuracy_mean_dataset_mean)
max_acc_ensemble = datos_alpha_s_1[idmax,'n_ensemble']
max_signifi = dif_no_sig_all[(dif_no_sig_all$valor_alpha == 2) & (dif_no_sig_all$valor_split == 1),'max_num_models'] 
# datos_alpha_s_1[datos_alpha_s_1$n_ensemble==max_signifi,'accuracy_mean_dataset_mean']


plot(datos_alpha_s_1$n_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean, type='l', xlab='n ensembles', ylab = 'accuracy mean', main ='alpha = 2, split =1')
points(max_acc_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_acc_ensemble], col='darkorange1', pch=19)
points(max_signifi, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_signifi], col='blue', pch=19)

```

The accuracy associated to each point is:

```{r}
# El máximo en cada caso es
print(paste('Accuracy blue dot:', round(datos_alpha_s_1[datos_alpha_s_1$n_ensemble==max_signifi,'accuracy_mean_dataset_mean'],4)))
print(paste('Accuracy orange dot:', round(max(datos_alpha_s_1$accuracy_mean_dataset_mean),4)))
```

Let's now make the same graph over the 40 combinations of alpha and split.

### Common legend

-   **Orange point**: It represents the number of ensembles with the highest accuracy value
-   **Blue point**: It indicates the maximum number of ensembles without significant differences with higher number of ensembles
-   **X axis**: Number of ensembles.
-   **Y axis**: Accuracy (average over dataset and complexity measures and cross validation)

```{r fig.width=16, fig.height=10, fig.align = 'center'}

df_ranking <- data.frame(df_filtered$valores_combo)
colnames(df_ranking) <- 'valores_combo'
df_ranking$valor_split <- df_filtered$valor_split
df_ranking$valor_alpha <- df_filtered$valor_alpha
df_ranking$max_total <- rep(NA,dim(df_ranking)[1])
df_ranking$max_no_signif <- rep(NA,dim(df_ranking)[1])

# Configuración de la cuadrícula (5 filas y 8 columnas)
par(mfrow = c(5, 8), mar = c(2, 2, 2, 1))

max_acc_max_ensemble = 0

# Bucles para alpha y split
for (alpha_value in c(2, 4, 6, 8, 10)) {
  for (split_value in c(1, 2, 4, 6, 8, 10, 12, 14)) {
    # Filtrar los datos por alpha y split
    datos_alpha_s_1 <- datos_alpha_s %>% filter(alpha == alpha_value, split == split_value)
    datos_alpha_s_1 <- as.data.frame(datos_alpha_s_1)
    datos_alpha_s_1$n_cycle <- as.numeric(as.character(datos_alpha_s_1$n_cycle))
    datos_alpha_s_1$n_ensemble <- as.numeric(as.character(datos_alpha_s_1$n_ensemble))
    
    # Encontrar el máximo
    idmax <- which.max(datos_alpha_s_1$accuracy_mean_dataset_mean)
    max_acc_ensemble <- datos_alpha_s_1[idmax, 'n_ensemble']
    # Guardamos para ranking
    df_ranking[(df_ranking$valor_alpha == alpha_value) & (df_ranking$valor_split == split_value),'max_total'] = max(datos_alpha_s_1$accuracy_mean_dataset_mean)

    # Cuántas veces el máximo accuracy se logra con el máximo número de modelos
    max_acc_max_ensemble = max_acc_max_ensemble + sum(max_acc_ensemble== max(datos_alpha_s_1[,'n_ensemble']))
    max_signifi <- dif_no_sig_all[(dif_no_sig_all$valor_alpha == alpha_value) & (dif_no_sig_all$valor_split == split_value), 'max_num_models']
    # Guardamos para ranking
    df_ranking[(df_ranking$valor_alpha == alpha_value) & (df_ranking$valor_split == split_value),'max_no_signif'] = max(datos_alpha_s_1[datos_alpha_s_1$n_ensemble <= max_signifi,'accuracy_mean_dataset_mean'])
    
    # Graficar
    plot(datos_alpha_s_1$n_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean, type = 'l', 
         xlab = 'n ensembles', ylab = 'accuracy mean', main = paste('alpha =', alpha_value, 'split =', split_value))
    
    # Añadir los puntos correspondientes
    points(max_acc_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_acc_ensemble], col = 'darkorange1', pch = 19)
    points(max_signifi, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_signifi], col = 'blue', pch = 19)
  }
}

# Restablecer los parámetros gráficos
par(mfrow = c(1, 1))


```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

The same plot but with equal y axis to enable fair comparisons.

```{r fig.width=16, fig.height=10, fig.align = 'center'}
# Configuración de la cuadrícula (5 filas y 8 columnas)
par(mfrow = c(5, 8), mar = c(2, 2, 2, 1))

# Bucles para alpha y split
for (alpha_value in c(2, 4, 6, 8, 10)) {
  for (split_value in c(1, 2, 4, 6, 8, 10, 12, 14)) {
    # Filtrar los datos por alpha y split
    datos_alpha_s_1 <- datos_alpha_s %>% filter(alpha == alpha_value, split == split_value)
    datos_alpha_s_1 <- as.data.frame(datos_alpha_s_1)
    datos_alpha_s_1$n_cycle <- as.numeric(as.character(datos_alpha_s_1$n_cycle))
    datos_alpha_s_1$n_ensemble <- as.numeric(as.character(datos_alpha_s_1$n_ensemble))
    
    # Encontrar el máximo
    idmax <- which.max(datos_alpha_s_1$accuracy_mean_dataset_mean)
    max_acc_ensemble <- datos_alpha_s_1[idmax, 'n_ensemble']
    max_signifi <- dif_no_sig_all[(dif_no_sig_all$valor_alpha == alpha_value) & (dif_no_sig_all$valor_split == split_value), 'max_num_models']
    
    # Graficar
    plot(datos_alpha_s_1$n_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean, type = 'l', 
         xlab = 'n ensembles', ylab = 'accuracy mean', main = paste('alpha =', alpha_value, 'split =', split_value),ylim=c(0.810,0.8153))
    
    # Añadir los puntos correspondientes
    points(max_acc_ensemble, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_acc_ensemble], col = 'darkorange1', pch = 19)
    points(max_signifi, datos_alpha_s_1$accuracy_mean_dataset_mean[datos_alpha_s_1$n_ensemble == max_signifi], col = 'blue', pch = 19)
  }
}

# Restablecer los parámetros gráficos
par(mfrow = c(1, 1))


```

Low split values with hight alpha values is not recommended (split 1 with alpha = 6,8,10, split 2 with alpha 10). They obtain the lower accuracy performances. --\> omit split = 1 for the range of recommended parameters.

For the rest of values, similar patterns are found.

We now plot all the lines in the same plot. The one obtaining clearly lower accuracy values is split = 1 and alpha = 10. The rest of values are visually super close and moving in a really close range. This indicates that any combination of the parameters is adequate.

```{r}

datos_alpha_s$n_ensemble <- as.numeric(as.character(datos_alpha_s$n_ensemble))
datos_alpha_s$accuracy_mean_dataset_mean <- as.numeric(as.character(datos_alpha_s$accuracy_mean_dataset_mean))

p <- plot_ly()

for (alpha_value in c(2, 4, 6, 8, 10)) {
  for (split_value in c(1, 2, 4, 6, 8, 10, 12, 14)) {
    datos_alpha_s_1 <- datos_alpha_s %>% filter(alpha == alpha_value, split == split_value)
    p <- p %>%
      add_lines(x = datos_alpha_s_1$n_ensemble, 
                y = datos_alpha_s_1$accuracy_mean_dataset_mean, 
                name = paste("alpha =", alpha_value, "split =", split_value), 
                line = list(width = 2),
                hovertemplate = paste('Alpha: ', alpha_value, 
                                    ' Split:', alpha_value,
                                    '<br>N ensemble:', datos_alpha_s_1$n_ensemble,
                                    '<br>Accuracy:', round(datos_alpha_s_1$accuracy_mean_dataset_mean,4),
                                    '<extra></extra>'))
  }
}

p <- p %>%
  layout(title = 'All combinations of alpha and split',
         xaxis = list(title = 'n ensembles'),
         yaxis = list(title = 'accuracy mean'),
         legend = list(title = list(text = 'Legend')))

p
```

Let's now obtain a ranking according to the maximum accuracy (orange point) and another according to the maximum accuracy obtain before no significant differences.

```{r}
df_ranking_order <- df_ranking  %>% arrange(desc(max_total))
df_ranking_order_sig <- df_ranking  %>% arrange(desc(max_no_signif))
df_ranking$max_total_order = rank(-df_ranking$max_total)
df_ranking$max_no_signif_order = rank(-df_ranking$max_no_signif)
```

```{r}
knitr::kable(df_ranking %>% arrange(max_total_order))
```

```{r}
cor.test(df_ranking$max_total_order, df_ranking$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking$max_total_order, df_ranking$max_no_signif_order, method=c('pearson'))
```

Both rankings do not have a high correlation but they generally agree in terms of what gets the worst results (alpha high and split low). The lack of agreement on which is best can be interpreted as meaning that different combinations of parameters work well.

For now, it is not clear what is better: lots of short cycles or few of long cycles. It seems that an intermediate point between those two states is adequate.

## Analysis per Complexity Measure

Now, we repeat the same analysis but for each complexity measure.

```{r}
#setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm_cycles")
# Data disaggregated per complexity measures
datos_CM <- read.csv('df_summary_CM.csv') 
str(datos_CM)
# Como en python empezamos en 0, tenemos que sumar 1 a n_ensemble
datos_CM$n_ensemble <- datos_CM$n_ensemble + 1
# Convert id and time into factor variables
datos_CM <- datos_CM %>%
  convert_as_factor(weights, n_cycle,n_ensemble)
```

```{r}
datos_CM_filtro <- datos_CM %>% filter(alpha<12, split <16) 
# No hace falta agregar más porque este dataset ya está agregado en origen
```

For the information about significant differences, we use the values obtain in general. That is, we are not repeating the multiple comparisons analyses per each complexity measure.

```{r}
plot_2max_grid_with_ranking <- function(CM,df_filtered,dif_no_sig_all,datos_CM){
  df_ranking_CM <- data.frame(df_filtered$valores_combo)
colnames(df_ranking_CM) <- 'valores_combo'
df_ranking_CM$valor_split <- df_filtered$valor_split
df_ranking_CM$valor_alpha <- df_filtered$valor_alpha
df_ranking_CM$max_total <- rep(NA,dim(df_ranking_CM)[1])
df_ranking_CM$max_no_signif <- rep(NA,dim(df_ranking_CM)[1])

# Configuración de la cuadrícula (5 filas y 8 columnas)
par(mfrow = c(5, 8), mar = c(2, 2, 2, 1))

max_acc_max_ensemble = 0

# Bucles para alpha y split
for (alpha_value in c(2, 4, 6, 8, 10)) {
  for (split_value in c(1, 2, 4, 6, 8, 10, 12, 14)) {
    # Filtrar los datos por alpha y split
    datos_CM_case <- datos_CM %>% filter(weights == CM,
                                         alpha == alpha_value, split == split_value)
    datos_CM_case <- as.data.frame(datos_CM_case)
    datos_CM_case$n_cycle <- as.numeric(as.character(datos_CM_case$n_cycle))
    datos_CM_case$n_ensemble <- as.numeric(as.character(datos_CM_case$n_ensemble))
    
    # Encontrar el máximo
    idmax <- which.max(datos_CM_case$accuracy_mean_mean)
    max_acc_ensemble <- datos_CM_case[idmax, 'n_ensemble']
    # Guardamos para ranking
    df_ranking_CM[(df_ranking_CM$valor_alpha == alpha_value) & (df_ranking_CM$valor_split == split_value),'max_total'] = max(datos_CM_case$accuracy_mean_mean)
    
    # Cuántas veces el máximo accuracy se logra con el máximo número de modelos
    max_acc_max_ensemble = max_acc_max_ensemble + sum(max_acc_ensemble== max(datos_CM_case[,'n_ensemble']))
    max_signifi <- dif_no_sig_all[(dif_no_sig_all$valor_alpha == alpha_value) & (dif_no_sig_all$valor_split == split_value), 'max_num_models']
    # Guardamos para ranking
    df_ranking_CM[(df_ranking_CM$valor_alpha == alpha_value) & (df_ranking_CM$valor_split == split_value),'max_no_signif'] = max(datos_CM_case[datos_CM_case$n_ensemble <= max_signifi,'accuracy_mean_mean'])
    
    # Graficar
    plot(datos_CM_case$n_ensemble, datos_CM_case$accuracy_mean_mean, type = 'l', 
         xlab = 'n ensembles', ylab = 'accuracy mean', main = paste('alpha =', alpha_value, 'split =', split_value),ylim=c(0.805,0.818))
    
    # Añadir los puntos correspondientes
    points(max_acc_ensemble, datos_CM_case$accuracy_mean_mean[datos_CM_case$n_ensemble == max_acc_ensemble]+0.0003, col = 'darkorange1', pch = 19)
    points(max_signifi, datos_CM_case$accuracy_mean_mean[datos_CM_case$n_ensemble == max_signifi], col = 'blue', pch = 19)
  }
}

# Restablecer los parámetros gráficos
par(mfrow = c(1, 1))
return(list(df_ranking_CM = df_ranking_CM,max_acc_max_ensemble = max_acc_max_ensemble))
}

```

```{r}
plot_all_combinations <- function(CM,datos_CM_filtro){
  datos_CM_filtro$n_ensemble <- as.numeric(as.character(datos_CM_filtro$n_ensemble))
datos_CM_filtro$accuracy_mean_mean <- as.numeric(as.character(datos_CM_filtro$accuracy_mean_mean))

p <- plot_ly()

for (alpha_value in c(2, 4, 6, 8, 10)) {
  for (split_value in c(1, 2, 4, 6, 8, 10, 12, 14)) {
    datos_CM_case <- datos_CM_filtro %>% filter(weights == CM,
      alpha == alpha_value, split == split_value)
    p <- p %>%
      add_lines(x = datos_CM_case$n_ensemble, 
                y = datos_CM_case$accuracy_mean_mean, 
                name = paste("alpha =", alpha_value, "split =", split_value), 
                line = list(width = 2),
                hovertemplate = paste('Alpha: ', alpha_value, 
                                    ' Split:', split_value,
                                    '<br>N ensemble:', datos_CM_case$n_ensemble,
                                    '<br>Accuracy:', round(datos_CM_case$accuracy_mean_mean,4),
                                    '<extra></extra>'))
  }
}

p <- p %>%
  layout(title = paste(CM,': All combinations of alpha and split'),
         xaxis = list(title = 'n ensembles'),
         yaxis = list(title = 'accuracy mean'),
         legend = list(title = list(text = 'Legend')))

p
}
```

### CLD

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'CLD'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### DCP

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'DCP'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### F1

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'F1'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### Hostility

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'Hostility'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### kDN

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'kDN'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### LSC

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'LSC'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### N1

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'N1'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r fig.width=16, fig.height=10, fig.align = 'center'}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### N2

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'N2'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

### TD_U

```{r fig.width=16, fig.height=10, fig.align = 'center'}
CM = 'TD_U'
res = plot_2max_grid_with_ranking(CM,df_filtered,dif_no_sig_all,datos_CM)
df_ranking_CM = res$df_ranking_CM
max_acc_max_ensemble = res$max_acc_max_ensemble
```

From the total of 40 combinations, in `r max_acc_max_ensemble` of them the maximum accuracy is obtained at the maximum number of models tested.

```{r}
plot_all_combinations(CM,datos_CM_filtro)
```

```{r}
df_ranking_CM$max_total_order = rank(-df_ranking_CM$max_total)
df_ranking_CM$max_no_signif_order = rank(-df_ranking_CM$max_no_signif)

cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('spearman'))
cor.test(df_ranking_CM$max_total_order, df_ranking_CM$max_no_signif_order, method=c('pearson'))

```

```{r}
knitr::kable(df_ranking_CM %>% arrange(max_total_order))
```

#### General conclusiones of the analysis per complexity measure

-   Worst results are found for extreme values of the parameters: split = 1 and high alphas or high alpha and split (but the first situation is worst). This is expected since we are splitting the complexity spectrum only in three pieces (with split=1: easy-base-hard) and we are multiplying the easy and the hard case for a really high value ( high alpha). Thus, instead of training with the situation easy-base-hard, we're training with the situation super easy - base - super hard and, consequently, the complexity spectrum is not correctly covered.

-   For every complexity measure, there are different values of the parameters that offer the best solution. They all have generally in common that intermediate values of alpha and split are better. But there are some exceptions, for example alpha and split equal to 10 works well for Hostility. From this, we can conclude that intermediate length of cycles are better than too short (s=1) or too long (s=10) cycles.

-   We have a total of 40 combinations of alpha and split values. For that 40 cases, the higher accuracy is obtained at the maximum number of models tested (around 300) in 6.5 cases (on average for all the complexity measures). In particular:

    -   CLD: 4 times

    -   DCP: 10 times

    -   F1: 11 times

    -   Hostility: 4 times

    -   kDN: 9 times

    -   LSC: 3 times

    -   N1: 7 times

    -   N2: 5 times

    -   TDU: 6 times

-   Regarding the correlation (both in terms of Pearson and Spearman correlation) between the ranking of alpha-split combinations according to the higher accuracy obtained when considering all the ensembles tested and when only considering those number of ensembles for which there are significant differences: in general the correlation is medium - low, indicating that there is no a clear agreement between those significant differences and the real maximum. Anyway, we have to compare what are the real differences in terms of if it is really worthy to keep training more and more models for maybe an increase of 0.001 in accuracy (differences are not significant and all graphs have shown that the accuracy is moving always in a close interval of values). In particular:

    -   CLD: corr 0.388

    -   DCP: corr = 0.72

    -   F1: corr = 0.31

    -   Hostility: corr = 0.449

    -   kDN: corr = 0.471

    -   LSC: corr = 0.5679

    -   N1: corr = 0.561

    -   N2: corr = 0.589

    -   TDU: corr = 0.687

        -   To deal with this, we are going to compare our method with SOTA methods in two different situations:

            -   Considering all the ensembles tested

            -   Considering only those where there are significant differences

AHORA AQUÍ TENGO QUE RESUMIR A PARTIR DE CUANDO NO HAY DIFERENCIAS SIGNIFICATIVAS PARA CADA CASO Y RELACIONARLO CON EL NÚMERO DE ENSEMBLES

LUEGO HACER LO MISMO PARA STD Y CON ELLO FILTRAR N_CYCLES

¿Qué es mejor: ciclos cortos o largos?
