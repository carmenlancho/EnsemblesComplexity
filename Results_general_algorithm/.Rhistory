log(0)
log2(0)
-log2(0.1)
-log2(0.9)
-log2(c(0.5,0.6,0.7,0.8,0.9,1))
ĺog2(0.3)
-log2(0.3)
-0.33*log2(0.33)-0.33*log2(0.33)-0.33*log2(0.33)
-0.25*log2(0.25)-0.25*log2(0.25)-0.25*log2(0.25)-0.25*log2(0.25)
-0.25*log(0.25)-0.25*log(0.25)-0.25*log(0.25)-0.25*log(0.25)
-log2(0.25)
-log2(0.33)
-0.5*log2(0.5)-0.25*log2(0.25)-0.1*log2(0.1)-0.15*log2(0.15)
-log2(1)
library(MASS)
library(mnormt)
library(mvtnorm)
library(pracma) # to integrate
# Function to calculate overlapping between two bivariate normal distributions
normal_overlap_2D <- function(mu11,mu12,mu21,mu22,sd11,sd12,sd21,sd22){
# Minimum of both distributions
min.f1f2_2D <- function(x,y) {
mu11 = 0
mu12 = 0
mu21 = 8
mu22 = 0
Sigma1 <- matrix(c(80,0,0,5),2,2)
Sigma2 <- matrix(c(8,0,0,8),2,2)
xs = matrix(cbind(c(x,y)),ncol=2)
f1 = dmvnorm(xs, mean=c(mu11,mu12),Sigma1)
f2 = dmvnorm(xs, mean=c(mu21,mu22),Sigma2)
pmin(f1, f2)
}
# we obtain the integral approximation with two different functions to double check
ovl1 = dblquad(Vectorize(min.f1f2_2D), -200, 200, -200, 200)
ovl2 = integral2(min.f1f2_2D,  -200, 200, -200, 200)
# danger: outside some limits the integrate approximations fail
return(list(ovl1,ovl2))
}
normal_overlap_2D()
# Function to calculate overlapping between two bivariate normal distributions
normal_overlap_2D <- function(mu11,mu12,mu21,mu22,sd11,sd12,sd21,sd22){
# Minimum of both distributions
min.f1f2_2D <- function(x,y) {
# normal parameters are changed here
mu11 = 0
mu12 = 0
mu21 = 8
mu22 = 0
Sigma1 <- matrix(c(80,0,0,5),2,2)
Sigma2 <- matrix(c(8,0,0,8),2,2)
xs = matrix(cbind(c(x,y)),ncol=2)
f1 = dmvnorm(xs, mean=c(mu11,mu12),Sigma1)
f2 = dmvnorm(xs, mean=c(mu21,mu22),Sigma2)
pmin(f1, f2)
}
# we obtain the integral approximation with two different functions to double check
ovl1 = dblquad(Vectorize(min.f1f2_2D), -200, 200, -200, 200)
ovl2 = integral2(min.f1f2_2D,  -200, 200, -200, 200)
# danger: outside some limits the integrate approximations fail
return(list(ovl1,ovl2))
}
normal_overlap_2D()
5*13
5*12
5*10
550*4
40*16
40*16*13
8320/60
139/24
640/60
5*28
9*5
12*5
7*5
9*5
12*5
7*5
306/4
9*20
12*20
7*20
13*20
mean(c(26, 27.5, 31, 28 ,25.5 ,30.5 ,32 ,31.5))
sum(c(26, 27.5, 31, 28 ,25.5 ,30.5 ,32 ,31.5))/8
((2.17*0.8)/0.9)²
((2.17*0.8)/0.9)^2
((2.17*0.8)/0.9)**2
((2.17*0.8)/0.9)
((2.17*2.8)/0.9)**2
4*(6/7)-1
16/7
17/7
3*6-14
3*(6/7)-2
18/7
18-14
4/7
(1/6)*(1/6)*((5/6)**4)
56/3
0.24*4
48*9*200
64*9*200
179-36
mean(c(0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101))
mean(c(0.82608696, 0.87681159, 0.77372263, 0.84671533, 0.79562044,
0.86131387, 0.8540146 , 0.86861314, 0.91240876, 0.91240876))
describe(iris)
library(Hmisc)
describe(iris)
describe(iris)
exp(2.373)/(1+exp(2.373))
exp(2.373)/(1+exp(2.373))
a = 1.846
exp(a)/(1+exp(a))
a = -0.791
exp(a)/(1+exp(a))
a = 3.4
exp(a)/(1+exp(a))
## Libraries
library(dplyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyverse)
library(emmeans)
library(optimx)
56+35
USArrests
prcomp(USArrests)
prcomp(USArrests, scale = TRUE)
prcomp(USArrests, scale = TRUE)$s
prcomp(USArrests, scale = TRUE)$x
pnorm(0.975)
qnorm(0.975)
0.317 + qnorm(0.975)*0.01695
library(dplyr)
library(MASS)
# Muestreo
# TRAIN TEST VALIDATION
numero_total = nrow(Boston)
set.seed(123456) # reproductivilidad
# 50% para TRAIN
indices_train = sample(1:numero_total, .5*numero_total)
Boston_train = Boston[indices_train,]
# 25% para TEST
indices = seq(1:numero_total)
indices_test = sample(indices[-indices_train], .25*numero_total)
Boston_test = Boston[indices_test,]
# 25% para VALIDATION
indices_validation = indices[-c(indices_train,indices_test)]
# OJO... Estos datos sólo los usaremos el último día de clase
# en el último minuto... y responderá a la pregunta siguiente:
# ¿Cómo va a funcionar nuestro modelo cuando lleguen nuevos datos
# al sistema?
Boston_validation = Boston[indices_validation,]
# Respuesta
Boston_train =
Boston_train %>%
mutate(precio = as.factor(medv>=30), log_crim=log(crim),sq_lstat = sqrt(lstat) )
Boston_train2 =
Boston_train %>%
mutate(s_crim=scale(log_crim),
s_lstat = scale(sq_lstat),
s_tax = scale(tax),
s_rad = scale(rad),
s_indus = scale(indus)
)
pca1 = princomp(Boston_train2[,c("s_crim",
"s_lstat",
"s_tax",
"s_rad",
"s_indus")])
summary(pca1)
# peso de cada variable en cada componente
pca1$loadings
# ahora vamos a pintar los puntos
plot(pca1$scores, col=as.numeric(Boston_train2$precio)+1,pch=19)
cmds1= cmdscale(dist(Boston_train2[,c("s_crim",
"s_lstat",
"s_tax",
"s_rad",
"s_indus")]),eig=TRUE)
cmds1
# plot solution
x <- cmds1$points[,1]
y <- cmds1$points[,2]
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Metric MDS", type="n")
text(x, y, labels = row.names(Boston_train2), cex=.7)
Boston_train2
USArrests
prcomp(USArrests)
plot(prcomp(USArrests))
summary(prcomp(USArrests))
prcomp(USArrests, scale = TRUE)
plot(prcomp(USArrests,scale=T))
summary(prcomp(USArrests,scale=T))
plot(prcomp(USArrests,scale=T)$x[,1:2])
plot(prcomp(USArrests,scale=T)$x[,1:2],type="n")
text(prcomp(USArrests,scale=T)$x[,1:2],rownames(USArrests))
biplot(prcomp(USArrests,scale=T))
sqrt((-0.5358995)**2 +(-0.5831836)**2 + (-0.2781909)**2 + (-0.5434321)**2)
corr(USArrests)
cor(USArrests)
USArrests
View(USArrests)
USArrests
prcomp(USArrests)
prcomp(USArrests, scale = TRUE)
plot(prcomp(USArrests,scale=T))
summary(prcomp(USArrests,scale=T))
plot(prcomp(USArrests,scale=T)$x[,1:2],type="n")
text(prcomp(USArrests,scale=T)$x[,1:2],rownames(USArrests))
biplot(prcomp(USArrests,scale=T))
mean(c(0.3,0.4,0.6))
mean(c(0.5,0.2,0.2))
mean(c(0,0.2,0.2))
1-0.43333
mean(c(0.4,0.2,0.2))
0.018+0.122
18/146
122/146
qnormal(0.975)
qnorm(0.975)
qnorm(0.9725)
1-0.025
qnorm(0.975)
0.5*0.5
z = qnorm(0.975)
std = 0.25 # var 0.5
e = 0.1
n = (z*std/e)²
n = (z*std/e)**2
n
e = 0.05
n = (z*std/e)**2
n
(1.96*1.5/0.05)**2
(1.64*0.25/0.05)**2
sqrt(0.5)
0.25*0.25
0.7071068*0.7071068
n = (z*sqrt(std)/e)**2
n
(1.64*sqrt(0.5)/0.05)**2
z
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.9,N=Inf)
library(samplingbook)
install.packages("samplingbook")
library(samplingbook)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.9,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
n = (z*sqrt(std)/e)**2
n
z = qnorm(0.95)
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.972,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.90,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.85,N=Inf)
z = qnorm(0.95)
z
1-0.05/2
z = qnorm(0.975)
var = 0.5 # var 0.5
e = 0.05
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.95,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
alpha = 0.05
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
n = ((z*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
alpha = 0.1
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n= ((z*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
alpha = 0.1
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n= ((z*sqrt(std))/e)^2
n
alpha = 0.1
var = 0.5 # var 0.5
e = 0.05
n = ((qnorm(1-alpha/2)*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.90,N=Inf)
e = 0.1
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.1
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.80,N=Inf)
alpha = 0.2
e = 0.15
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.11
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.11,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.12,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.13,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.8,N=Inf)
sample.size.mean(e=0.11,S=sqrt(0.5),level=0.8,N=Inf)
sample.size.mean(e=0.115,S=sqrt(0.5),level=0.8,N=Inf)
alpha = 0.2
e = 0.115
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.11
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.115,S=sqrt(0.5),level=0.8,N=Inf)
80+60+38+70
248/16
11+18
85910+14000+213000+25000+11000+12000+858400+80000+53000
install.packages("wskm")
library(wskm)
# The data twkm.sample has 2000 objects and 410 variables.
# Scale the data before clustering
x <- scale(twkm.sample[,1:409])
twkm.sample
# Group information is formated as below.
# Each group is separated by ';'.
strGroup <- "0-75;76-291;292-355;356-402;403-408"
strGroup
groups <- c(rep(0, abs(0-75-1)), rep(1, abs(76-291-1)), rep(2, abs(292-355-1)),
rep(3, abs(356-402-1)), rep(4, abs(403-408-1)))
groups
# Use the twkm algorithm.
mytwkm <- twkm(x, 10, strGroup, 3, 1, seed=19)
mytwkm
mytwkm2 <- twkm(x, 10, groups, 3, 1, seed=19)
mytwkm2
all.equal(mytwkm, mytwkm2)
# You can print the clustering result now.
mytwkm$cluster
mytwkm$featureWeight
str(x)
dim(x)
mytwkm$featureWeight
length(mytwkm$featureWeight)
mytwkm$groupWeight
mytwkm$iterations
mytwkm$restarts
mytwkm$totiters
mytwkm$totss
# Use a cluster validation method from package 'fpc'.
# real.cluster is the real class label of the data 'twkm.sample'.
real.cluster <- twkm.sample[,410]
real.cluster
# cluster.stats() computes several distance based statistics.
kmstats <- cluster.stats(d=dist(x), as.integer(mytwkm$cluster), real.cluster)
kmstats
# corrected Rand index
kmstats$corrected.rand
# variation of information (VI) index
kmstats$vi
seq(0,409,1)
group_prueba <- c(seq(0,409,1))
# Use the twkm algorithm.
mytwkm <- twkm(x, 10, group_prueba, 3, 1, seed=19)
group_prueba <- c(seq(1,409,1))
group_prueba
# Use the twkm algorithm.
mytwkm <- twkm(x, 10, group_prueba, 3, 1, seed=19)
mytwkm
# You can print the clustering result now.
mytwkm$cluster
mytwkm$featureWeight
mytwkm$groupWeight
mytwkm <- twkm(x, 10, strGroup, 3, 1, seed=19)
mytwkm$featureWeight
# Use the twkm algorithm.
mytwkm <- twkm(x, 10, group_prueba, 3, 1, seed=19)
mytwkm$featureWeight
dim(x)
twkm.sample
dim(twkm.sample)
x
# The data fgkm.sample has 600 objects and 50 dimensions.
# Scale the data before clustering
x <- scale(fgkm.sample)
fgkm.sample
dim(fgkm.sample)
strGroup <- "0-9;10-19;20-49"
groups <- c(rep(0, 10), rep(1, 10), rep(2, 30))
# Use the fgkm algorithm.
myfgkm <- fgkm(x, 3, strGroup, 3, 1, seed=19)
myfgkm2 <- fgkm(x, 3, groups, 3, 1, seed=19)
all.equal(myfgkm, myfgkm2)
# You can print the clustering result now.
myfgkm$cluster
myfgkm$featureWeight
myfgkm$groupWeight
group_prueba <- c(seq(1,50,1))
# Use the fgkm algorithm.
myfgkm <- fgkm(x, 3, group_prueba, 3, 1, seed=19)
myfgkm
myfgkm$featureWeight
myewkm <- ewkm(iris[1:4], 3, lambda=0.5, maxiter=100)
myewkm
plot(iris[1:4], col=myewkm$cluster)
# For comparative testing
mykm <- kmeans(iris[1:4], 3)
mykm
plot(iris[1:4], col=mykm$cluster)
myewkm <- ewkm(scale(iris[1:4]), 3, lambda=0.5, maxiter=100)
myewkm$weights
4754/5000
18800/20000
18804/20000
90/5
99+18
library(wskm)
twkm.sample
iris
myewkm <- ewkm(scale(iris[1:4]), 3, lambda=0.5, maxiter=100)
myewkm
myewkm <- ewkm(iris[1:4], 3)
myewkm
myewkm$weights
sum(c(2.219989e-01,   0.2320880, 2.367765e-01 ,3.091366e-01))
knitr::opts_chunk$set(echo = TRUE)
library(palmerpenguins)
summary(penguins)
hist(penguins$body_mass_g)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm")
datos <- read.csv('SummarizeResults_ParameterConfiguration_CDB.csv')
str(datos)
datos <- datos %>%
convert_as_factor(Dataset, alpha,split, weights)
datos %>%
group_by(alpha, split) %>%
shapiro_test(accuracy_mean_std)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_mean ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov, correction = 'GG')
pwc_split <- datos %>%
pairwise_t_test(
accuracy_mean_std ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc_split
datos %>%
group_by(split) %>%
summarise_at(vars(accuracy_mean_std),
list(Mean_split = mean))
# the higher the value of split, the higher the variance
pwc_alpha <- datos %>%
pairwise_t_test(
accuracy_mean_std ~ alpha, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc_alpha
datos %>%
group_by(alpha) %>%
summarise_at(vars(accuracy_mean_std),
list(Mean_alpha = mean))
# the higher the value of alpha, the higher the variance
library(readr)
urlfile="https://raw.githubusercontent.com/IsaacMartindeDiego/IA/master/datasets/california_housing.csv"
mydata<-read_csv(url(urlfile))
head(mydata)
