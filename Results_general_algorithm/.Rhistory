return(list(ovl1,ovl2))
}
normal_overlap_2D()
# Function to calculate overlapping between two bivariate normal distributions
normal_overlap_2D <- function(mu11,mu12,mu21,mu22,sd11,sd12,sd21,sd22){
# Minimum of both distributions
min.f1f2_2D <- function(x,y) {
# normal parameters are changed here
mu11 = 0
mu12 = 0
mu21 = 8
mu22 = 0
Sigma1 <- matrix(c(80,0,0,5),2,2)
Sigma2 <- matrix(c(8,0,0,8),2,2)
xs = matrix(cbind(c(x,y)),ncol=2)
f1 = dmvnorm(xs, mean=c(mu11,mu12),Sigma1)
f2 = dmvnorm(xs, mean=c(mu21,mu22),Sigma2)
pmin(f1, f2)
}
# we obtain the integral approximation with two different functions to double check
ovl1 = dblquad(Vectorize(min.f1f2_2D), -200, 200, -200, 200)
ovl2 = integral2(min.f1f2_2D,  -200, 200, -200, 200)
# danger: outside some limits the integrate approximations fail
return(list(ovl1,ovl2))
}
normal_overlap_2D()
5*13
5*12
5*10
550*4
40*16
40*16*13
8320/60
139/24
640/60
5*28
9*5
12*5
7*5
9*5
12*5
7*5
306/4
9*20
12*20
7*20
13*20
mean(c(26, 27.5, 31, 28 ,25.5 ,30.5 ,32 ,31.5))
sum(c(26, 27.5, 31, 28 ,25.5 ,30.5 ,32 ,31.5))/8
((2.17*0.8)/0.9)²
((2.17*0.8)/0.9)^2
((2.17*0.8)/0.9)**2
((2.17*0.8)/0.9)
((2.17*2.8)/0.9)**2
4*(6/7)-1
16/7
17/7
3*6-14
3*(6/7)-2
18/7
18-14
4/7
(1/6)*(1/6)*((5/6)**4)
56/3
0.24*4
48*9*200
64*9*200
179-36
mean(c(0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.87681159, 0.87681159,
0.87681159, 0.87681159, 0.87681159, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101,
0.55797101, 0.55797101, 0.55797101, 0.55797101, 0.55797101))
mean(c(0.82608696, 0.87681159, 0.77372263, 0.84671533, 0.79562044,
0.86131387, 0.8540146 , 0.86861314, 0.91240876, 0.91240876))
describe(iris)
library(Hmisc)
describe(iris)
describe(iris)
exp(2.373)/(1+exp(2.373))
exp(2.373)/(1+exp(2.373))
a = 1.846
exp(a)/(1+exp(a))
a = -0.791
exp(a)/(1+exp(a))
a = 3.4
exp(a)/(1+exp(a))
## Libraries
library(dplyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyverse)
library(emmeans)
library(optimx)
56+35
USArrests
prcomp(USArrests)
prcomp(USArrests, scale = TRUE)
prcomp(USArrests, scale = TRUE)$s
prcomp(USArrests, scale = TRUE)$x
pnorm(0.975)
qnorm(0.975)
0.317 + qnorm(0.975)*0.01695
library(dplyr)
library(MASS)
# Muestreo
# TRAIN TEST VALIDATION
numero_total = nrow(Boston)
set.seed(123456) # reproductivilidad
# 50% para TRAIN
indices_train = sample(1:numero_total, .5*numero_total)
Boston_train = Boston[indices_train,]
# 25% para TEST
indices = seq(1:numero_total)
indices_test = sample(indices[-indices_train], .25*numero_total)
Boston_test = Boston[indices_test,]
# 25% para VALIDATION
indices_validation = indices[-c(indices_train,indices_test)]
# OJO... Estos datos sólo los usaremos el último día de clase
# en el último minuto... y responderá a la pregunta siguiente:
# ¿Cómo va a funcionar nuestro modelo cuando lleguen nuevos datos
# al sistema?
Boston_validation = Boston[indices_validation,]
# Respuesta
Boston_train =
Boston_train %>%
mutate(precio = as.factor(medv>=30), log_crim=log(crim),sq_lstat = sqrt(lstat) )
Boston_train2 =
Boston_train %>%
mutate(s_crim=scale(log_crim),
s_lstat = scale(sq_lstat),
s_tax = scale(tax),
s_rad = scale(rad),
s_indus = scale(indus)
)
pca1 = princomp(Boston_train2[,c("s_crim",
"s_lstat",
"s_tax",
"s_rad",
"s_indus")])
summary(pca1)
# peso de cada variable en cada componente
pca1$loadings
# ahora vamos a pintar los puntos
plot(pca1$scores, col=as.numeric(Boston_train2$precio)+1,pch=19)
cmds1= cmdscale(dist(Boston_train2[,c("s_crim",
"s_lstat",
"s_tax",
"s_rad",
"s_indus")]),eig=TRUE)
cmds1
# plot solution
x <- cmds1$points[,1]
y <- cmds1$points[,2]
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
main="Metric MDS", type="n")
text(x, y, labels = row.names(Boston_train2), cex=.7)
Boston_train2
USArrests
prcomp(USArrests)
plot(prcomp(USArrests))
summary(prcomp(USArrests))
prcomp(USArrests, scale = TRUE)
plot(prcomp(USArrests,scale=T))
summary(prcomp(USArrests,scale=T))
plot(prcomp(USArrests,scale=T)$x[,1:2])
plot(prcomp(USArrests,scale=T)$x[,1:2],type="n")
text(prcomp(USArrests,scale=T)$x[,1:2],rownames(USArrests))
biplot(prcomp(USArrests,scale=T))
sqrt((-0.5358995)**2 +(-0.5831836)**2 + (-0.2781909)**2 + (-0.5434321)**2)
corr(USArrests)
cor(USArrests)
USArrests
View(USArrests)
USArrests
prcomp(USArrests)
prcomp(USArrests, scale = TRUE)
plot(prcomp(USArrests,scale=T))
summary(prcomp(USArrests,scale=T))
plot(prcomp(USArrests,scale=T)$x[,1:2],type="n")
text(prcomp(USArrests,scale=T)$x[,1:2],rownames(USArrests))
biplot(prcomp(USArrests,scale=T))
mean(c(0.3,0.4,0.6))
mean(c(0.5,0.2,0.2))
mean(c(0,0.2,0.2))
1-0.43333
mean(c(0.4,0.2,0.2))
0.018+0.122
18/146
122/146
qnormal(0.975)
qnorm(0.975)
qnorm(0.9725)
1-0.025
qnorm(0.975)
0.5*0.5
z = qnorm(0.975)
std = 0.25 # var 0.5
e = 0.1
n = (z*std/e)²
n = (z*std/e)**2
n
e = 0.05
n = (z*std/e)**2
n
(1.96*1.5/0.05)**2
(1.64*0.25/0.05)**2
sqrt(0.5)
0.25*0.25
0.7071068*0.7071068
n = (z*sqrt(std)/e)**2
n
(1.64*sqrt(0.5)/0.05)**2
z
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.9,N=Inf)
library(samplingbook)
install.packages("samplingbook")
library(samplingbook)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.9,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
n = (z*sqrt(std)/e)**2
n
z = qnorm(0.95)
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.972,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.90,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.85,N=Inf)
z = qnorm(0.95)
z
1-0.05/2
z = qnorm(0.975)
var = 0.5 # var 0.5
e = 0.05
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.95,N=Inf)
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
alpha = 0.05
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n = (z*sqrt(std)/e)**2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.95,N=Inf)
n = ((z*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
alpha = 0.1
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n= ((z*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
alpha = 0.1
z = qnorm(1-alpha/2)
var = 0.5 # var 0.5
e = 0.05
n= ((z*sqrt(std))/e)^2
n
alpha = 0.1
var = 0.5 # var 0.5
e = 0.05
n = ((qnorm(1-alpha/2)*sqrt(std))/e)^2
n
sample.size.mean(e=0.05,S=sqrt(0.5),level=0.90,N=Inf)
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.90,N=Inf)
e = 0.1
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.1
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.80,N=Inf)
alpha = 0.2
e = 0.15
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.11
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.11,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.12,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.13,S=sqrt(0.5),level=0.85,N=Inf)
sample.size.mean(e=0.1,S=sqrt(0.5),level=0.8,N=Inf)
sample.size.mean(e=0.11,S=sqrt(0.5),level=0.8,N=Inf)
sample.size.mean(e=0.115,S=sqrt(0.5),level=0.8,N=Inf)
alpha = 0.2
e = 0.115
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
alpha = 0.2
e = 0.11
var = 0.5 # var 0.5
n = ((qnorm(1-alpha/2)*sqrt(var))/e)^2
n
sample.size.mean(e=0.115,S=sqrt(0.5),level=0.8,N=Inf)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm")
datos <- read.csv('complex_info_total.csv')
datos
datos <- read.csv('SummarizeResults_ParameterConfiguration_CDB.csv')
datos
str(datos)
datos <- datos %>%
convert_as_factor(Dataset, alpha,split, weights)
str(datos)
datos %>%
group_by(alpha, split,weights) %>%
shapiro_test(accuracy_mean_std)
datos %>%
group_by(alpha, split) %>%
shapiro_test(accuracy_mean_std)
ggqqplot(datos, "accuracy_mean_std", ggtheme = theme_bw()) +
facet_grid(alpha ~ split, labeller = "label_both")
res.aov <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(alpha, split)
)
rlang::last_trace()
get_anova_table(res.aov)
res.aov <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(weights, split)
)
res.aov <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(weights, split,alpha)
)
8 <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(weights, split,alpha)
)
res.aov
get_anova_table(res.aov, correction = 'GG')
res.aov <- anova_test(
data = datos, dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
res.aov <- anova_test(
data = datos, dv = accuracy_mean_median, wid = Dataset,
within = c(weights, split,alpha)
)
res.aov <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm")
datos <- read.csv('SummarizeResults_ParameterConfiguration_CDB.csv')
str(datos)
# Convert id and time into factor variables
datos <- datos %>%
convert_as_factor(Dataset, alpha,split, weights)
datos %>%
group_by(alpha, split) %>%
shapiro_test(accuracy_mean_std)
res.aov <- anova_test(
data = datos, dv = accuracy_mean_std, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov)
get_anova_table(res.aov, correction = 'auto')
get_anova_table(res.aov, correction = 'none')
get_anova_table(res.aov, correction = 'GG')
res.aov <- anova_test(
data = datos, dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_mean ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov, correction = 'GG')
library(tidyverse)
library(ggpubr)
library(rstatix)
library(datarium)
getwd()
setwd("/home/carmen/PycharmProjects/EnsemblesComplexity/Results_general_algorithm")
datos <- read.csv('SummarizeResults_ParameterConfiguration_CDB.csv')
str(datos)
datos <- datos %>%
convert_as_factor(Dataset, alpha,split, weights)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_mean ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov, correction = 'GG')
View(datos)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_mdiean ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
res.aov <- anova_test(
data = datos,formula = accuracy_mean_median ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov, correction = 'GG')
res.aov <- anova_test(
data = datos,formula = accuracy_mean_std ~ weights*split*alpha,
dv = accuracy_mean_mean, wid = Dataset,
within = c(weights, split,alpha)
)
get_anova_table(res.aov, correction = 'GG')
pwc <- datos %>%
pairwise_t_test(
accuracy_mean_std ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc
View(pwc)
pwc2 <- values %>%
group_by(alpha,weights) %>%
pairwise_t_test(
score ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc2 <- datos %>%
group_by(alpha,weights) %>%
pairwise_t_test(
score ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc2 <- datos %>%
group_by(alpha,weights) %>%
pairwise_t_test(
accuracy_mean_std ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc2
View(pwc2)
pwc_split <- datos %>%
pairwise_t_test(
accuracy_mean_std ~ split, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc_split
pwc_alpha <- datos %>%
pairwise_t_test(
accuracy_mean_std ~ alpha, paired = TRUE,
p.adjust.method = "bonferroni"
)
pwc_alpha
View(pwc_alpha)
View(pwc_split)
datos %>%
group_by(split) %>%
mean()
datos %>%
group_by(split) %>%
summarise_at(vars(accuracy_mean_std),
list(Mean_split = mean))
datos %>%
group_by(alpha) %>%
summarise_at(vars(accuracy_mean_std),
list(Mean_alpha = mean))
m1b = lmer(accuracy_mean_std ~ 1 + weights + split + alpha +
(1 | Dataset), datos)
library(dplyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyverse)
library(emmeans)
library(optimx)
library(ggsignif)
m1b = lmer(accuracy_mean_std ~ 1 + weights + split + alpha +
(1 | Dataset), datos)
summary(m1b)
confint(m1b)
emm = emmeans(m1b, ~ split)
pairs(emm)
plot(fitted(m1b), resid(m1b, type = "pearson")) #
abline(0,0, col="red")
qqnorm(resid(m1b))
qqline(resid(m1b), col = "red") # add a perfect fit line
# random intercept subject
qqnorm(ranef(m1b)$Subject[,1] )
m1b = lmer(accuracy_mean_std ~ 1 + weights + split + alpha +
(1 + weights + split + alpha | Dataset), datos)
